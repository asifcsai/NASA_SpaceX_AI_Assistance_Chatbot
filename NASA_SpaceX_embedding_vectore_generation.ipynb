{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becb621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_core.runnables import RunnableParallel , RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pathlib import Path\n",
    "# LangChain - Document class can be in different places depending on LC version\n",
    "try:\n",
    "    from langchain.schema import Document        # newer versions\n",
    "except Exception:\n",
    "    from langchain.docstore.document import Document  # fallback\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826710ca",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8424410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Config you can tweak\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_KEY'] = 'replace by your api'\n",
    "PAPERS_DIR = Path(r\"E:\\Programming\\Artificial Intelligence\\Generative AI\\Chatbot\\NASA-SpaceX\\research_paper\")\n",
    "CHUNK_SIZE = 1000      # characters per chunk (adjust later)\n",
    "CHUNK_OVERLAP = 200    # characters overlap between chunks\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "PERSIST_DIR = Path(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4f90d",
   "metadata": {},
   "source": [
    "Load one JSON file (manual, inspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94537a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists: True\n",
      "{\n",
      "  \"file\": \"0001-mice-in-bion-m-1-space-mission-training-and-selection.html\",\n",
      "  \"source_url\": \"\",\n",
      "  \"images\": [\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/banners/logo-plosone.png\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/70e25d36bcd9/pone.0104830.g001.jpg\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/baff4e1e20fe/pone.0104830.g002.jpg\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/b536fa0d195d/pone.0104830.g003.jpg\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/2943f07a32f9/pone.0104830.g004.jpg\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/551344689a57/pone.0104830.g005.jpg\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/624d20564868/pone.0104830.g006.jpg\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/6e518b32c174/pone.0104830.g007.jpg\",\n",
      "    \"https://cdn.ncbi.nlm.nih.gov/pmc/blobs/fa56/4136787/899daebf315b/pone.0104830.g008.jpg\"\n",
      "  ],\n",
      "  \"text\": \"PLoS One . 2014 Aug 18;9(8):e104830. doi: 10.1371/journal.pone.0104830 Mice in Bion-M 1 Space Mission: Training and Selection Alexander Andreev-Andrievskiy Alexander Andreev-Andrievskiy 1 Institute for Biomedical Problems, Russian Academy of Sciences, Mos\n"
     ]
    }
   ],
   "source": [
    "# step_2_open_one_json.py\n",
    "sample_path = PAPERS_DIR / \"0001-mice-in-bion-m-1-space-mission-training-and-selection.json\"\n",
    "print(\"Exists:\", sample_path.exists())\n",
    "\n",
    "text = sample_path.read_text(encoding=\"utf-8\")\n",
    "# Show first 800 chars so you can inspect\n",
    "print(text[:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e21eb5",
   "metadata": {},
   "source": [
    "Write load_papers_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09289fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 607 document(s) from E:\\Programming\\Artificial Intelligence\\Generative AI\\Chatbot\\NASA-SpaceX\\research_paper\n",
      "First doc metadata: {'source_file': '0001-mice-in-bion-m-1-space-mission-training-and-selection.json', 'paper_file': '0001-mice-in-bion-m-1-space-mission-training-and-selection.html'}\n",
      "First doc text (first 400 chars):\n",
      "PLoS One . 2014 Aug 18;9(8):e104830. doi: 10.1371/journal.pone.0104830 Mice in Bion-M 1 Space Mission: Training and Selection Alexander Andreev-Andrievskiy Alexander Andreev-Andrievskiy 1 Institute for Biomedical Problems, Russian Academy of Sciences, Moscow, Russia 2 Moscow State University, Biology Faculty, Moscow, Russia Find articles by Alexander Andreev-Andrievskiy 1, 2, * , Anfisa Popova Anf\n"
     ]
    }
   ],
   "source": [
    "# step_3_load_function.py\n",
    "def load_papers_from_json(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    docs = []\n",
    "    for path in sorted(folder.glob(\"*.json\")):\n",
    "        try:\n",
    "            raw = path.read_text(encoding=\"utf-8\")\n",
    "            data = json.loads(raw)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {path.name}: parse error -> {e}\")\n",
    "            continue\n",
    "\n",
    "        # 1) Prefer a root \"text\"\n",
    "        text = None\n",
    "        if isinstance(data.get(\"text\"), str) and data.get(\"text\").strip():\n",
    "            text = data[\"text\"].strip()\n",
    "        # 2) else, fallback to the first chunk's text\n",
    "        elif \"chunks\" in data and isinstance(data[\"chunks\"], list) and len(data[\"chunks\"]) > 0:\n",
    "            first_chunk = data[\"chunks\"][0]\n",
    "            text = first_chunk.get(\"text\", \"\").strip()\n",
    "        else:\n",
    "            print(f\"Skipping {path.name}: no usable text found.\")\n",
    "            continue\n",
    "\n",
    "        # 3) Build metadata so we always know the source\n",
    "        metadata = {\n",
    "            \"source_file\": path.name,\n",
    "            \"paper_file\": data.get(\"file\", \"\")\n",
    "        }\n",
    "\n",
    "        # 4) Create a LangChain Document\n",
    "        doc = Document(page_content=text, metadata=metadata)\n",
    "        docs.append(doc)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} document(s) from {folder}\")\n",
    "    return docs\n",
    "\n",
    "# run it\n",
    "paper_docs = load_papers_from_json(PAPERS_DIR)\n",
    "# Inspect first doc\n",
    "if paper_docs:\n",
    "    print(\"First doc metadata:\", paper_docs[0].metadata)\n",
    "    print(\"First doc text (first 400 chars):\")\n",
    "    print(paper_docs[0].page_content[:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649990da",
   "metadata": {},
   "source": [
    "Chunk documents and inspect chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f032d6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_docs\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# run it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m chunked_docs = \u001b[43mchunk_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# inspect first 3 chunks\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunked_docs[:\u001b[32m3\u001b[39m]):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mchunk_documents\u001b[39m\u001b[34m(paper_docs, chunk_size, chunk_overlap)\u001b[39m\n\u001b[32m     11\u001b[39m source = md.get(\u001b[33m\"\u001b[39m\u001b[33msource_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# create a safe chunk_id using length of current list for that source\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# get current count for this source\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m existing_count = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m new_docs \u001b[38;5;28;01mif\u001b[39;00m d.metadata.get(\u001b[33m\"\u001b[39m\u001b[33msource_file\u001b[39m\u001b[33m\"\u001b[39m) == source)\n\u001b[32m     15\u001b[39m md[\u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisting_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m new_doc = Document(page_content=doc.page_content, metadata=md)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     11\u001b[39m source = md.get(\u001b[33m\"\u001b[39m\u001b[33msource_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# create a safe chunk_id using length of current list for that source\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# get current count for this source\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m existing_count = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m new_docs \u001b[38;5;28;01mif\u001b[39;00m \u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msource_file\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m)\n\u001b[32m     15\u001b[39m md[\u001b[33m\"\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexisting_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m new_doc = Document(page_content=doc.page_content, metadata=md)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# step_4_chunking.py\n",
    "def chunk_documents(paper_docs, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_docs = splitter.split_documents(paper_docs)   # returns list[Document]\n",
    "    # Improve metadata: add paper filename and a per-paper chunk id\n",
    "    new_docs = []\n",
    "    for doc in split_docs:\n",
    "        md = dict(doc.metadata or {})\n",
    "        # if splitter created chunk-level metadata it might not have chunk ids;\n",
    "        # we'll create a readable chunk id using the source_file.\n",
    "        source = md.get(\"source_file\", \"unknown\")\n",
    "        # create a safe chunk_id using length of current list for that source\n",
    "        # get current count for this source\n",
    "        existing_count = sum(1 for d in new_docs if d.metadata.get(\"source_file\") == source)\n",
    "        md[\"chunk_id\"] = f\"{source}_chunk_{existing_count}\"\n",
    "        new_doc = Document(page_content=doc.page_content, metadata=md)\n",
    "        new_docs.append(new_doc)\n",
    "    print(f\"Chunked into {len(new_docs)} chunks (chunk_size={chunk_size}, overlap={chunk_overlap}).\")\n",
    "    return new_docs\n",
    "\n",
    "# run it\n",
    "import pickle\n",
    "\n",
    "CHUNKED_DOCS_FILE = Path(\"chunked_docs.pkl\")\n",
    "\n",
    "if CHUNKED_DOCS_FILE.exists():\n",
    "    print(\"Loading chunked documents from disk...\")\n",
    "    with open(CHUNKED_DOCS_FILE, \"rb\") as f:\n",
    "        chunked_docs = pickle.load(f)\n",
    "    print(f\"Loaded {len(chunked_docs)} chunks from {CHUNKED_DOCS_FILE}\")\n",
    "else:\n",
    "    print(\"Chunked file not found. Splitting paper documents into chunks...\")\n",
    "    chunked_docs = chunk_documents(paper_docs, chunk_size=500, chunk_overlap=100)\n",
    "    # Save to disk for future runs\n",
    "    with open(CHUNKED_DOCS_FILE, \"wb\") as f:\n",
    "        pickle.dump(chunked_docs, f)\n",
    "    print(f\"Saved {len(chunked_docs)} chunks to {CHUNKED_DOCS_FILE}\")\n",
    "\n",
    "\n",
    "# # inspect first 3 chunks\n",
    "# for i, c in enumerate(chunked_docs[:3]):\n",
    "#     print(\"---- chunk\", i, \"metadata:\", c.metadata)\n",
    "#     print(c.page_content[:300].replace(\"\\n\", \" \"), \"\\n\")\n",
    "# Load chunked_docs without recomputing\n",
    "with open(CHUNKED_DOCS_FILE, \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(chunked_docs)} chunks from {CHUNKED_DOCS_FILE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f887742",
   "metadata": {},
   "source": [
    "Build a FAISS vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce7011",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PERSIST_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# step_5_build_faiss.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_vector_store\u001b[39m(docs, persist_dir=\u001b[43mPERSIST_DIR\u001b[49m, model_name=EMBEDDING_MODEL):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitializing embeddings with model:\u001b[39m\u001b[33m\"\u001b[39m, model_name)\n\u001b[32m      4\u001b[39m     embeddings = HuggingFaceEmbeddings(model_name=model_name)  \u001b[38;5;66;03m# may download model\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'PERSIST_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# step_5_build_faiss.py\n",
    "def build_vector_store(docs, persist_dir=PERSIST_DIR, model_name=EMBEDDING_MODEL):\n",
    "    print(\"Initializing embeddings with model:\", model_name)\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)  # may download model\n",
    "    print(\"Embedding model ready. Creating FAISS index (embedding documents)...\")\n",
    "    vector_store = FAISS.from_documents(docs, embeddings)\n",
    "    # Save to disk so you can load later without recomputing\n",
    "    persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        vector_store.save_local(str(persist_dir))\n",
    "        print(\"Saved FAISS index to\", persist_dir)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: save_local failed - trying persist()\", e)\n",
    "        try:\n",
    "            vector_store.persist(str(persist_dir))\n",
    "            print(\"Persisted FAISS index to\", persist_dir)\n",
    "        except Exception as e2:\n",
    "            print(\"Warning: persisting failed:\", e2)\n",
    "    return vector_store\n",
    "\n",
    "# run it (this may take time)\n",
    "if PERSIST_DIR.exists() and any(PERSIST_DIR.iterdir()):\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    vs = FAISS.load_local(\n",
    "        str(PERSIST_DIR),\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    print('loaded successfully.')\n",
    "else:\n",
    "    print(\"Building FAISS index for the first time...\")\n",
    "    vs = build_vector_store(chunked_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9836555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# vec = emb.embed_query(\"Hello world\")\n",
    "# print(len(vec))  # should print 384\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2717e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa0e994",
   "metadata": {},
   "source": [
    "Quick retrieval test (see that embeddings work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_6_retrieval_test.py\n",
    "query = \"mice behavior after space flight\"  # change to your test query\n",
    "# Option A: simple FAISS method (langchain wrapper)\n",
    "results = vs.similarity_search(query, k=3)   # returns list[Document]\n",
    "print(\"Found\", len(results), \"chunks.\")\n",
    "for r in results:\n",
    "    print(\"-> source:\", r.metadata.get(\"source_file\"), \"| chunk_id:\", r.metadata.get(\"chunk_id\"))\n",
    "    print(r.page_content[:400].replace(\"\\n\", \" \"), \"\\n---\\n\")\n",
    "\n",
    "# Option B: use retriever API (fixed deprecation warning)\n",
    "retriever = vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "docs = retriever.invoke(query)  # Changed from get_relevant_documents to invoke\n",
    "print(\"Retriever returned:\", len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319121ed",
   "metadata": {},
   "source": [
    "Load persisted FAISS later (no recompute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_7_load_saved.py - FIXED: Added allow_dangerous_deserialization=True\n",
    "emb = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)  # you still need embeddings object\n",
    "vs2 = FAISS.load_local(\n",
    "    str(PERSIST_DIR), \n",
    "    emb, \n",
    "    allow_dangerous_deserialization=True  # Added this parameter\n",
    ")\n",
    "print(\"Loaded index. nb vectors:\", vs2.index.ntotal if hasattr(vs2.index, \"ntotal\") else \"unknown\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7411029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_8_format_docs.py\n",
    "def format_docs(retrieved_docs):\n",
    "    pieces = []\n",
    "    for doc in retrieved_docs:\n",
    "        src = doc.metadata.get(\"source_file\", \"unknown\")\n",
    "        pieces.append(f\"[{src}] {doc.page_content.strip()}\")\n",
    "    return \"\\n\\n---\\n\\n\".join(pieces)\n",
    "\n",
    "ctx = format_docs(results)\n",
    "print(ctx[:2000])\n",
    "print('Done-')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
